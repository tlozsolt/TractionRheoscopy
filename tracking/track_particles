import sys
sys.path.extend(['../particleLocating'])
from particleLocating import dplHash_v2 as dpl
import trackpy as tp
import pandas as pd
import numba
import numpy as np
import glob


@numba.vectorize
def sqDisp_1D(x0, xf):
    """Compute the displacement from x0 ti xf"""
    return (xf-x0)**2

@numba.vectorize
def disp3D(x0,xf,y0,yf,z0,zf):
    """ Compute the euclidean displacement"""
    return np.sqrt(np.sum(np.array([sqDisp_1D(x0,xf),\
                           sqDisp_1D(y0,yf),\
                           sqDisp_1D(z0,zf)])\
                          )\
                   )

@numba.vectorize
def elasticStrain(x0,xf,y0,yf,z0,zf):
    return disp3D(x0,xf,y0,yf,z0,zf)/z0

def compute_disp_numba(unstacked_DF,t0=0,tf=1):
   result = disp3D(unstacked_DF['x (um)'][t0].to_numpy(), unstacked_DF['x (um)'][tf].to_numpy(),\
                   unstacked_DF['y (um)'][t0].to_numpy(), unstacked_DF['y (um)'][tf].to_numpy(),\
                   unstacked_DF['z (um)'][t0].to_numpy(), unstacked_DF['z (um)'][tf].to_numpy())
   name_str = 'disp t: {} -> {}'.format(t0,tf)
   disp = pd.Series(result, index=unstacked_DF.index, name=name_str)
   unstacked_DF[disp.name] = disp
   return unstacked_DF

def compute_elasticStrain_numba(unstacked_DF,t0=0,tf=1):
    result = elasticStrain(unstacked_DF['x (um)'][t0].to_numpy(), unstacked_DF['x (um)'][tf].to_numpy(), \
                    unstacked_DF['y (um)'][t0].to_numpy(), unstacked_DF['y (um)'][tf].to_numpy(), \
                    unstacked_DF['z (um)'][t0].to_numpy(), unstacked_DF['z (um)'][tf].to_numpy())
    name_str = 'strain t: {} -> {}'.format(t0,tf)
    strain = pd.Series(result, index=unstacked_DF.index, name=name_str)
    unstacked_DF[strain.name] = strain
    return unstacked_DF


# unstacked.groupby(pd.cut(unstacked['z (um)'][0],np.arange(0,100,10)))['displacement'].describe()

def displacement_height(trajDF):
    """
    This function plots the displacements of the particles average over z-bins based on the initial
    z position. Incomplete trajectories are not included in the plot
    :param trajDF:
    :return:
    """
    # From the traj array, bin in z based on inital position (based on kwargs)
    # in each z-bin, determine the displacement vector across the desired time step
    # make some exceptions for traj that are not complete over the time gap
    # histogram the displacments
    # plot a histogrma of displacement for each height as a horizontal violin plot
    # I think this should have the option of returning a series to be plotted with seaborn, or
    # go ahead and plot and return the seaborn plt
    unstacked = trajDF.set_index(['particle','time'])[['x (um)', 'y (um)', 'z (um)']].unstack()

    # VERY SLOW becuase I really have just written a fancy for loop here and not vectorized anything
    # its possible that this could be improved with numba, but also possible that the fastest thing
    # would be to extract numpy arrays from pandas, run numba with vectorization and par loops
    # and repackage into pandas.
    #displacement = []
    #for i, traj in unstacked.iterrows():
    #    displacement.append(
    #        np.sqrt( np.sum( [(traj[key].loc[1] - traj[key].loc[0]) ** 2 for key in ['x (um)', 'y (um)', 'z (um)']])))
    #return displacment

    # much faster, but I dont know how to solve this without explicity calling it separately for each coord
    x_disp = unstacked['x (um)'].apply(lambda coord: (coord[1] - coord[0]) ** 2, axis=1)
    y_disp = unstacked['y (um)'].apply(lambda coord: (coord[1] - coord[0]) ** 2, axis=1)
    z_disp = unstacked['z (um)'].apply(lambda coord: (coord[1] - coord[0]) ** 2, axis=1)

    # now compute the displacement
    #disp = np.sqrt()

    return unstacked, x_disp,y_disp, z_disp

def kilfoil2DataFrame(locationFile, time,
                      out_cols = ('x (um)', 'y (um)', 'z (um)', 'mass (px intensity)','frame'),
                      px2Micron = (0.115,0.115,0.15), convert2Micron=True, addTime=True):
    """ Take a kilfoil location text output and return a dataframe compatibile with trackpy"""
    df = pd.read_csv(locationFile,sep='\t',
                     usecols=[0,1,2,3],
                     names=['x (px)', 'y (px)', 'z (px)', 'mass (px intensity)'])
    if addTime == True: df['frame'] = time
    if convert2Micron == True:
        df['x (um)'] = px2Micron[0]*df['x (px)']
        df['y (um)'] = px2Micron[1]*df['y (px)']
        df['z (um)'] = px2Micron[2]*df['z (px)']
    return df[list(out_cols)]

def location_CSV_2_mem(path,fName_regExp='t*_xyz_coordinates.txt', t_range=[0,25]):
    """
    -> Scans over a directory and search string
    -> if frame number is in t_range:
    ->   reads kilfoil location file to dataFrame
    ->   converts to um and outputs default out_cols
    ->   apends the dataframe to output list
    -> return pd.concat(output list)
    :param path:
    :param fName_regExp:
    :param t_range:
    :return:
    """
    fName_list = glob.glob(path + '/' + fName_regExp)
    fName_list.sort()
    t_min = t_range[0]
    t_max = t_range[1]
    output_df = []
    for locationFile in fName_list:
        t = int(locationFile.split(sep='/')[-1].split(sep='_')[0][1:5])
        if t>=t_min and t<=t_max: output_df.append(kilfoil2DataFrame(locationFile, t))
        print("time point {} added to locationDF".format(t))
    return pd.concat(output_df)


def location_CSV_2_HDF5(path, fName_regExp='t*_xyz_coordinates.txt', outPath ='/home/zsolt/buf' ):
    # get all the files in the directory you want to include
    # loop over the files
    #   read them in to pandas from csv
    #   add additional columns for um positions
    #   maybe drop the pixel positions to save on memory
    #   add a time column specific to the file
    #   put into hdf5 store with trackpy hdf.
    #   print some updates
    # return path to dhf5 file
    with tp.PandasHDFStoreBig(outPath +'/'+ 'tfrGel09102018b_shearRun09232018b.h5') as s:
        # loop over the files
        fName_list = glob.glob(path + '/' + fName_regExp)
        fName_list.sort()
        # I think we might need to sort since the files will be placed sequentially in hdf5
        # but we not need to
        for locationFile in fName_list:
            # parse the file to get time point
            t = int(locationFile.split(sep='/')[-1].split(sep='_')[0][1:5])
            df = kilfoil2DataFrame(locationFile, t)
            s.put(df)
            print("location time {} added to hdf5!".format(t))
    return None

def linkTraj_HDF5(h5File):
    with tp.PandasHDFStoreBig(h5File) as s:
        for linked in tp.link_iter(s, 0.8, memory=2, pos_columns=['x (um)', 'y (um)', 'z (um)'], t_column='frame'):
            s.put(linked)

    # now this displacment need to be added to traj array in some way...I think it should be added to
    # unstacked and that is the most natural data structure...but I dont know how to deal with the multi-index
    #...at least I dont know yet.
    # what we need here is a merge/join on particle id, possibly with a modification of x_disp to include
    # a multi index of the same dimensions as unstacked.


    # now you can bin in z using groupby and plot displacment distributions.

def hist_trajLength(trajDF,output='figure'):
    """
    Returns a histogram object of the lengths of trajectories in trajDF
    :param trajDF:
    :return:
    """
    particle_id_counts = trajDF['particle'].value_counts()
    trajLength_hist = sns.distplot(particle_id_counts, kde = False)
    trajLength_hist.set(xlabel = 'Length of trajectory', ylabel = 'counts')
    if output = 'figure': return trajLength_hist
    elif output == 'file':
        path = input('Enter a path and filename to save histogram of traj lengths:')
        trajLength_hist.figure.savefig(path)
        return print('Figure saved to {}'.format(path))
    else:
        print("output in hist_trajLength must be \'figure\' or \'file\'")
        raise KeyError

def plotTrajectory(trajDF, length):
    """
    Plots all the trajectories of a certian legnth
    :param trajDF:
    :param length:
    :return:
    """
    particle_id_counts = trajDF['particle'].value_counts()
    complete_traj = trajDF[trajDF['particle'].isin(particle_id_counts[particle_id_counts == length].index)]

    # get the statistics on the z position of trajcectories of given lenghth
    complete_traj.groupby('particle')['z (um)'].describe()

    # plot trajectories by looping over particle id keys
    for particle_id in complete_traj.groupby('particle').groups.keys():
        # select the particle trajectory
        traj = trajDF [trajDF['particle'] == particle_id]
        # take the initial value in y
        start = traj['y (um)'].values[0]
        # get the height
        height = traj['z (um)'].values[0]
        sns.lineplot(np.arange(0,length,1),traj['y (um)'].apply(lambda y: (y - start)))

# ToDo:
#   [ ] Implement wrapper on cKDTree to compute nnb shell and return distances in parallel on pandas df
#   [ ] Common operations that I need to do on the traj or location data
#       -> For each particle, compute something on its nnb shell and return
#          a particle property (local coordination for example)
#       -> Bin particle positions in x,y,z and return average of particle properties
#          where the average is taken over the bin and the bin is:
#          + spatial, like z-slices or xyz bins
#          + structural: connected components or grains or something to that effect
#          + possibly a rolling window where the bins are overlapping.
#          + or bin on neighborhood...like bin the particle, its nnb, and next nearest
#       -> Compute particle property that involved the particle at multiple time points
#          but indexed to a initial nnb configuration (ie strain defines nnb, and ref and current config)
#       -> compute spatial correlations of particle properties
#   [ ] Key concepts in pandas that make this possible:
#       -> groupby to select whatever property you want to aggregate and apply the analysis on
#          + groupby particle id -> unique trajectory
#          + groupby spatial bin -> spatial binning
#          + grouby by ovito structure (like fcc or hcp): structural binning
#          + groupby by all non-fcc, non-hcp in this region -> group by grain boundaries or defects.
#       -> apply to apply whatever function you have written on single particle or nnb or traj across the entire
#          DataFrame
#      -> stack/unstack: not sure what this accomplishes but it is used in trackpy analysis codes, has to do with
#         pivoting tables with multindices (like a given particle in a given bin would have a multindex of particle id
#         **and** bin id.) I think groupby is essentially a method of creating multi indices
#       -> as an exercise, to write your own g(r) code:
#          + bin spatially upto the cutoff lengths, probably 10 particles with overlaps
#          + compute search tree in each bin (parallel)
#          + using the search tree compute pairwise distance (parallel)
#          + do the same thing for all the time points to gain even more statistics
#          + collect all the distances, normalize to ideal gas, and plot g(r)
#          + for angle resolved, you have to keep track of orientation, not just distance
#            and bin on angles of those distance vectors.
#   [ ] What aspect of these problems have benn implemented in either ovito, freud, or other python
#       based matsci packages?
#       -> I bet that the basic implementation is the same for all the cases:
#          + spatial bin
#          + compute property, possibly constructing search tree to get nnb
#          + once you have the bins, parallelize over multiple cores as at the point
#            the analysis is embarrassingly parallel.
#          + the data structures are all the same: groupby and apply mapped onto traj array.
#          + in princple the analysis already available on freud or ovito can be combined with
#            eachother and analysis that I write based on pandas DataFrame output from trackpy.

if __name__ == "__main__":
    print("Import was successful")
    dplInst = dpl.dplHash('/home/zsolt/TractionRheoscopy/metaDataYAML/tfrGel09052019b_shearRun05062019i_metaData_scriptTesting.yaml')
    print(dplInst.queryHash(44))

    dplInst = dpl.dplHash('/home/zsolt/TractionRheoscopy/metaDataYAML/tfrGel09052019b_shearRun05062019i_metaData_scriptTesting.yaml')

    """
      ToDo:
        -> how does trackpy linking work, even on a subset of the data
        -> Does trackpy tracking work if you give it 1M particles?
        -> will it work better if we has the data? But then how will I stitich the tracks back together? I
           I have a hard time believing that trackpy doesnt carry out a spatial hash as Tom Caswell was the one
           who introduced me to the concept of spatial hashing.
    """

    """
    # load a data frame from a folder of positions
    location_path = '/mnt/serverdata/zsolt/ParticleLocations/tfrGel09102018b_shearRun09232018b/particleLocations'
    t0_fName = 't0000_xyz_coordinates.txt'
    t1_fName = 't0001_xyz_coordinates.txt'

    t0_df = pd.read_csv(location_path +'/'+ t0_fName,sep='\t', usecols=[0,1,2,3], names=['x (px)', 'y (px)', 'z (px)', 'mass (px intensity)'])
    # add time column
    t0_df['time'] = 0
    # convert to micron
    t0_df['x (um)'] = 0.115*t0_df['x (px)']
    t0_df['y (um)'] = 0.115*t0_df['y (px)']
    t0_df['z (um)'] = 0.15*t0_df['z (px)']

    # repeat for second time step
    t1_df = pd.read_csv(location_path +'/'+ t1_fName,sep='\t', usecols=[0,1,2,3], names=['x (px)', 'y (px)', 'z (px)', 'mass (px intensity)'])
    t1_df['time'] = 1
    t1_df['x (um)'] = 0.115*t1_df['x (px)']
    t1_df['y (um)'] = 0.115*t1_df['y (px)']
    t1_df['z (um)'] = 0.15*t1_df['z (px)']

    # try to run particle linking.
    traj = tp.link(pd.concat([t0_df,t1_df]), search_range=0.8, pos_columns=['x (um)', 'y (um)', 'z (um)'], t_column='time')
    print(traj.info)
    """
    locationPath ='/mnt/serverdata/zsolt/ParticleLocations/tfrGel09102018b_shearRun09232018b/particleLocations'

    locationDF_all = location_CSV_2_mem(locationPath,t_range=[0, 30])
    traj_all = tp.link_df(locationDF_all, search_range=0.9, adaptive_stop=0.05, adaptive_step=0.9, pos_columns=['x (um)', 'y (um)', 'z (um)'],
                       t_column='frame')


    #unstacked, disp_x, disp_y, disp_z = displacement_height(traj)
    #unstacked = traj.set_index(['particle','time'])[['x (um)', 'y (um)', 'z (um)']].unstack()
    #displacement = compute_disp_numba(unstacked)
    #strain = compute_elasticStrain_numba(unstacked)


